# Muon

Muon is a matrix-aware neural network optimizer that updates parameters at the **layer level** rather than only at the elementwise (per-weight) level. Instead of treating each weight as an independent scalar, Muon treats weight matrices as geometric objects and applies structured transformations that preserve or encourage desirable matrix properties (e.g., approximate orthogonality). This leads to large, stable steps and potentially faster convergence compared to standard optimizers like Adam or AdamW.

Muon is part of a broader family of **vector-based or matrix-based optimizers**, alongside things like K-FAC, Shampoo, and Natural Gradient methods.

---

## üß≠ Overview

Muon modifies weight matrices using **matrix-level transforms**, often involving normalization or orthogonalization-like operations. The idea is to produce ‚Äúhealthy‚Äù weight matrices with stable singular values, which improves training stability and allows using larger learning rates or taking fewer steps.

It is best known for achieving **state-of-the-art ‚Äúsmall record‚Äù training runs** (training models extremely quickly on small hardware budgets) and for pioneering structured update strategies that are more geometric than those found in per-parameter optimizers like Adam.

---

## üß© Core Concepts

- **Matrix-aware updates**: Instead of updating `W` via `W ‚Üê W ‚àí Œ∑*g`, Muon uses transforms such as  
  `W ‚Üê W ‚àí Œ∑ * f(W, g)`  
  where `f` includes matrix-level operations (e.g., symmetric normalization, orthogonalization).
- **Stabilized singular values**: Muon frequently constrains weight matrices to remain closer to orthonormal, improving gradient flow.
- **Structured scaling**: Updates are scaled consistently across the matrix rather than solely per element.
- **Layer-level preconditioning**: Each layer gets its own structured update rule.

---

## üìä Comparison Chart: Muon vs Other Optimizers

| Optimizer | Type | Structure-Aware | Memory | Use Case | Notes |
|----------|------|------------------|--------|----------|--------|
| **Muon** | Matrix-based | Yes | Medium‚ÄìHigh | Fast convergence, structured layers | Requires custom kernels for speed |
| **Adam** | Per-parameter adaptive | No | Medium | General-purpose | Standard default |
| **AdamW** | Adam + decoupled WD | No | Medium | Large models, better generalization | More stable with WD |
| **Lion** | Low-memory sign optimizer | No | Low | LLMs, constrained memory | Small state footprint |
| **K-FAC** | Approximate natural gradient | Yes | High | Research, curvature-aware | Expensive, complex |
| **Shampoo** | Preconditioned matrix optimizer | Yes | Very High | Vision, large layers | Heavy memory use |
| **SGD + Momentum** | Scalar | No | Low | Strong generalization | Sensitive to LR |

---

## ‚öôÔ∏è How Muon Works (High-Level)

### 1. Matrix-normalized update  
Muon computes a gradient-based update but then adjusts it using a structured operator that depends on the weight matrix.

### 2. Approximate orthogonalization  
Muon nudges weight matrices toward orthogonality, either implicitly through the update rule or with explicit normalization steps.

### 3. Per-layer geometric scaling  
Muon ensures the update respects the geometry of the layer (e.g., linear, convolutional). This avoids the ‚Äúper-parameter overfitting‚Äù behavior sometimes seen in Adam-like optimizers.

### 4. Large-step stable training  
Because the matrix structure keeps singular values well-behaved, Muon can take **larger steps per update**, reducing total training cost.

---

## üöÄ Practical Advantages

- **Fewer optimization steps needed** ‚Äî often trains faster in wall-clock or token count.
- **More stable singular value spectrum** leads to healthier gradients.
- **Implicit regularization via orthogonality** improves generalization.
- **Can outperform Adam/AdamW in small-run or experimental setups**.

---

## ‚ö†Ô∏è Limitations / Tradeoffs

- **More complex math** than Adam; not as beginner-friendly.
- Requires **specialized CUDA kernels** for fast operation (Flash-Muon and similar projects exist).
- **More memory** than simple methods due to matrix-level computations.
- Needs **careful tuning** when scaling to very large models (e.g., weight decay balancing, per-layer LR scaling).
- Distributed training is more challenging because **updates depend on entire matrices**, not scalars.

---

## üß™ Known Variants / Related Work

- **Flash-Muon**: optimized CUDA implementation using faster matrix primitives.  
- **Dion**: a related optimizer that adopts similar structured approaches.  
- **Natural Gradient / K-FAC**: conceptually related because they also use matrix-level geometry.  
- **Orthonormality-preserving methods**: Muon is part of this conceptual family.

---

## üõ†Ô∏è Developer Tools & Implementation Notes

- Implementations usually require batched matrix ops (e.g., SVD approximations, Newton‚ÄìSchulz iterations, or symmetric normalization).
- Flash-Muon provides fast kernels; without these, Muon can be **much slower** than Adam.
- PyTorch implementations exist in several experimental repositories.
- Works best when each layer‚Äôs matrix structure (shape, rank) is well understood and consistent.

---

## üìö Use Cases

- **Fast prototyping** when you need fewer training steps.  
- **Small-scale talent runs** (few GPUs, low tokens).  
- **Research into training dynamics**, matrix geometry, and alternative update rules.  
- **Tasks where orthogonality or stable singular values are important** (e.g., RNNs, certain transformer blocks).  

---

## üîß Strengths

- Large effective step sizes  
- Better conditioning of weights  
- Matrix-level understanding of layer geometry  
- Potentially fewer total training tokens  

---

## ‚ùó Weaknesses

- More complicated implementation  
- Computationally heavier without specialized kernels  
- Harder to scale to extremely large models  
- Requires tuning of structured hyperparameters  

---

## üîó Related Concepts / Notes

- [[Vector Based Optimizers]]  
- [[Adam]]  
- [[AdamW]]  
- [[Lion]]  
- [[K-FAC]]  
- [[Natural Gradient]]  
- [[Shampoo Optimizer]]  
- [[CUDA Kernels]]  
- [[Distributed Training]]  

---

## üèÅ Summary

Muon is a powerful and conceptually elegant optimizer that treats weight matrices as the structured geometric objects they are. By enforcing healthier matrix properties and enabling larger, more stable updates, Muon often converges faster than standard per-parameter optimizers. However, its complexity, compute demands, and tuning requirements mean it is currently best suited to researchers and advanced practitioners rather than general deployment.
