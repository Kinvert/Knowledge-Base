# Replay Buffer

A **Replay Buffer** is a crucial data structure in reinforcement learning that stores past experiences (transitions) collected by the agent during interaction with the environment. It allows agents, especially those using off-policy algorithms, to sample and reuse experiences multiple times for more stable and efficient learning.

---

## üîç Overview

- Stores tuples of `(state, action, reward, next_state, done)` called **transitions** [[RL Transition]].  
- Enables **experience replay** [[Experience Replay]] by breaking correlations between sequential data samples.  
- Commonly used in off-policy algorithms such as [[DQN]], [[DDPG]], and [[SAC]].  
- Can be implemented as a fixed-size circular buffer to limit memory usage.  
- Supports various sampling strategies like uniform random sampling or prioritized replay.

---

## üß† Core Concepts

- **Buffer Size**: Maximum number of transitions stored; old data is overwritten when full.  
- **Sampling**: Random mini-batches sampled to train neural networks.  
- **[[Prioritized Experience Replay]]**: Samples important transitions more frequently based on temporal difference (TD) error.
- **Storage Format**: Efficient storage for large-scale data, often using numpy arrays or tensors.  
- **Off-Policy Learning**: Replay buffers decouple data collection from policy updates.

---

## üß∞ Use Cases

- Stabilizing training in value-based methods like [[Deep Q Learning]].  
- Improving sample efficiency by reusing experiences multiple times.  
- Enabling asynchronous and distributed RL by decoupling acting and learning.  
- Supporting multi-agent RL setups with shared or individual replay buffers.

---

## ‚úÖ Pros

- Reduces correlation in training data leading to better convergence.  
- Allows reuse of scarce or expensive data samples.  
- Facilitates parallel and distributed training architectures.  
- Can implement prioritized sampling to accelerate learning.

---

## ‚ùå Cons

- Adds memory overhead, especially for large environments.  
- May introduce bias if sampling is not representative of the current policy.  
- Complex prioritized schemes can increase computational cost.  
- Requires careful tuning of buffer size and sampling strategies.

---

## üìä Comparison Table: Replay Buffer Variants

| Variant                    | Sampling Method           | Advantages                   | Disadvantages                  |
|----------------------------|--------------------------|------------------------------|-------------------------------|
| Uniform Replay Buffer       | Uniform random sampling  | Simple, unbiased             | Inefficient for rare events    |
| Prioritized Experience Replay (PER) | Importance sampling based on TD error | Faster learning on important transitions | More complex, potential bias  |
| Episodic Replay Buffer      | Stores full episodes     | Maintains temporal context   | Higher memory use              |
| Hindsight Experience Replay (HER) | Re-labels goals post hoc | Improves sparse reward learning | Specific to goal-oriented tasks|

---

## üîß Compatible Items

- [[Deep Q Learning]] ‚Äì Classic algorithm using replay buffers  
- [[Off-Policy]] ‚Äì Category of algorithms relying on replay buffers  
- [[Experience Replay]] ‚Äì The technique enabled by replay buffers  
- [[TD Learning]] ‚Äì Updates often based on replay buffer samples  
- [[Neural Networks]] ‚Äì Function approximators trained using replay samples  

---

## üîó Related Concepts

- [[Replay Buffer Sampling]] ‚Äì Strategies for selecting data from buffer  
- [[Experience Replay]] ‚Äì Technique utilizing replay buffers  
- [[Replay Buffer Size]] ‚Äì Memory and performance trade-offs  
- [[Batch Learning]] ‚Äì Training on mini-batches sampled from the buffer  
- [[RL Policy]] ‚Äì Replay buffers store experiences generated by the policy  

---

## üìö Further Reading

- [Original DQN Paper - Mnih et al. 2015](https://www.nature.com/articles/nature14236)  
- [Prioritized Experience Replay - Schaul et al. 2015](https://arxiv.org/abs/1511.05952)  
- [Spinning Up - Experience Replay](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html)  
- [OpenAI Baselines Replay Buffer Implementation](https://github.com/openai/baselines)  

---
