# On-Policy

**On-Policy** refers to a class of reinforcement learning algorithms that learn and improve the policy using data collected exclusively from the current policy itself. The learning updates are based on experiences generated by the policy being optimized.

---

## üîç Overview

- The agent collects experience by following its **current policy**.  
- Learning and data collection are tightly coupled; new data is required for each update.  
- Typically less sample efficient than off-policy methods but often more stable.  
- Common in policy gradient methods such as [[PPO]], [[A2C]], and [[A3C]].  
- Suited for environments where fresh, up-to-date data improves convergence.

---

## üß† Core Concepts

- **Policy Evaluation and Improvement** happen on the same policy.  
- Uses trajectories generated by the current policy to update it directly.  
- No replay buffer needed, as data is not reused from older policies.  
- Exploration is embedded in the stochasticity of the policy.  
- Often relies on **Monte Carlo** or **GAE (Generalized Advantage Estimation)** for return estimation.

---

## üß∞ Use Cases

- Tasks requiring stable and reliable policy updates.  
- Environments where the distribution of data shifts quickly.  
- When interaction with the environment is cheap and fresh samples are easy to obtain.  
- Training agents in games, robotics simulations, and control where robustness is prioritized.

---

## ‚úÖ Pros

- Generally more stable and theoretically grounded updates.  
- Avoids issues with distribution mismatch seen in off-policy methods.  
- Simpler implementation without replay buffers or importance sampling.  
- Effective for many standard RL benchmarks.

---

## ‚ùå Cons

- Requires large amounts of fresh data, often leading to lower sample efficiency.  
- Can be slower to train in data-scarce or expensive-to-sample environments.  
- Exploration limited to current policy stochasticity, which might be insufficient.

---

## üìä Comparison Table: On-Policy vs Off-Policy

| Aspect            | On-Policy                         | [[Off-Policy]]                     |
| ----------------- | --------------------------------- | ---------------------------------- |
| Data Requirement  | Fresh data from current policy    | Can reuse past experience          |
| Sample Efficiency | Lower                             | Higher                             |
| Stability         | Generally stable                  | Can be unstable without correction |
| Examples          | PPO, A2C, A3C                     | DQN, DDPG, SAC, TD3                |
| Exploration       | Intrinsic in policy stochasticity | Separate exploration policy        |

---

## üîß Compatible Items

- [[Policy Gradient]] ‚Äì Most on-policy algorithms use this method  
- [[GAE]] (Generalized Advantage Estimation) ‚Äì Common advantage estimator  
- [[Monte Carlo Methods]] ‚Äì Used for return estimation  
- [[Stochastic Policy]] ‚Äì Core for exploration in on-policy learning  
- [[RL Episode]] ‚Äì On-policy methods often process complete episodes  

---

## üîó Related Concepts

- [[Off-Policy]] ‚Äì Complementary RL paradigm  
- [[Policy Evaluation]] ‚Äì Estimating value based on current policy  
- [[Advantage Function]] ‚Äì Measures relative value of actions  
- [[Exploration vs Exploitation]] ‚Äì Exploration is policy-driven  
- [[Temporal Difference Learning]] ‚Äì Sometimes combined with on-policy updates  

---

## üìö Further Reading

- [Reinforcement Learning: An Introduction - Sutton & Barto (Chapters on Policy Gradient and On-Policy Learning)](http://incompleteideas.net/book/the-book-2nd.html)  
- [PPO Paper - Schulman et al., 2017](https://arxiv.org/abs/1707.06347)  
- [A2C/A3C Paper - Mnih et al., 2016](https://arxiv.org/abs/1602.01783)  
- [Spinning Up - On-Policy Methods](https://spinningup.openai.com/en/latest/algorithms/ppo.html)  

---
