# Temporal Difference (TD) Learning

**Temporal Difference (TD) Learning** is a foundational method in reinforcement learning that blends ideas from **Monte Carlo methods** and **dynamic programming**. It allows agents to learn value functions directly from experience, without a full model of the environment, and without waiting for the final outcome of an episode.

---

## ðŸ” Overview

- TD learning estimates the value of a state by **bootstrapping**â€”updating predictions based on other learned predictions.  
- Unlike Monte Carlo methods, TD learning can update estimates after every step instead of waiting for an episode to finish.  
- Core to many popular RL algorithms like [[Q-Learning]], [[SARSA]], and [[Actor Critic]].  
- Enables **online learning** in real-time environments, making it suitable for robotics and games.  

---

## ðŸ§  Core Concepts

- **Bootstrapping**: Updating an estimate based partly on other learned estimates.  
- **TD Target**:  
  `TD Target = r + Î³ * V(sâ€²)`  
  where `r` is the reward, `Î³` the discount factor, and `V(sâ€²)` the estimated value of the next state.  
- **TD Error (Î´)**:  
  `Î´ = TD Target âˆ’ V(s)`  
  This error drives the update to the value function.  
- **Update Rule** (for state value):  
  `V(s) â† V(s) + Î± * Î´`  
- **n-step TD**: Uses rewards from multiple steps to form the target.  
- **TD(Î»)**: A generalization combining n-step methods using eligibility traces.  

---

## ðŸ§° Use Cases

- Learning value functions online from real-time sensor data.  
- Environments where episodes are long or continuous.  
- Robotics, video games, and any domain where step-wise feedback is available.  
- Forming the basis of value-based methods like [[Q-Learning]], [[SARSA]], and [[Actor Critic]].  

---

## âœ… Pros

- Learns directly from raw experience without requiring a model.  
- More sample-efficient than Monte Carlo methods.  
- Works well in online and incremental settings.  
- Can be combined with function approximation (e.g., neural nets).  

---

## âŒ Cons

- Bootstrapping can introduce bias.  
- Sensitive to hyperparameters like learning rate (Î±) and discount factor (Î³).  
- Less stable than Monte Carlo in some scenarios, especially early in training.  

---

## ðŸ“Š Comparison Table: TD vs Monte Carlo vs Dynamic Programming

| Property                   | Temporal Difference (TD) | Monte Carlo           | Dynamic Programming     |
|----------------------------|---------------------------|------------------------|--------------------------|
| Updates During Episode     | âœ… Yes                    | âŒ No                  | âŒ No                    |
| Needs Environment Model    | âŒ No                     | âŒ No                  | âœ… Yes                   |
| Variance                   | Low                       | High                   | Low                      |
| Bias                       | High                      | None                   | None (with exact model)  |
| Applicability to RL        | âœ… Broad                  | âœ… Broad               | âŒ Limited               |

---

## ðŸ”§ Compatible Items

- [[TD Learning]] â€“ They are the same
- [[Q-Learning]] â€“ A TD method that learns action-value functions off-policy  
- [[SARSA]] â€“ A TD method that learns on-policy  
- [[TD Error]] â€“ The core update signal in TD learning  
- [[Value Function]] â€“ Learned and updated using TD methods  
- [[Actor Critic]] â€“ Uses TD error to update both actor and critic  
- [[Eligibility Traces]] â€“ Enhance TD learning via TD(Î»)  

---

## ðŸ”— Related Concepts

- [[Reinforcement Learning]] (Core category including TD methods)  
- [[Monte Carlo Methods]] (Complementary approach to TD)  
- [[Policy Evaluation]] (TD is often used for this purpose)  
- [[Function Approximation]] (TD can work with neural nets, linear functions, etc.)  
- [[Bellman Equation]] (TD target is derived from this)  

---

## ðŸ“š Further Reading

- [Sutton & Barto â€“ Chapter 6: Temporal Difference Learning](http://incompleteideas.net/book/the-book.html)  
- [Spinning Up â€“ Value Learning](https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html#value-learning)  
- [CS50 RL â€“ Temporal Difference Learning Explained](https://cs50.harvard.edu/ai/2020/weeks/7/)  
- [TD(Î») and Eligibility Traces (David Silver)](https://www.davidsilver.uk/teaching/)  

---

## ðŸ—‚ Suggested Folder Location

Software > Reinforcement Learning > Core Concepts > Temporal Difference Learning  
