# Policy Iteration

**Policy Iteration** is a classical algorithm in reinforcement learning and dynamic programming used to compute the optimal policy for a Markov Decision Process (MDP). It alternates between **policy evaluation** (estimating how good a policy is) and **policy improvement** (updating the policy based on the evaluation), until convergence.

---

## üîç Overview

- Introduced by Richard Bellman as part of his work on dynamic programming.  
- Guarantees convergence to the optimal policy in a finite number of steps for MDPs with a finite number of states and actions.  
- Each iteration improves the policy until the best possible one is found.  
- Forms the theoretical basis of many modern RL algorithms.  

---

## üß† Core Concepts

- **Policy Evaluation**:  
  Calculate the value function `V(s)` for the current policy `œÄ` using the **Bellman Expectation Equation**.  
  This often requires iterative updates until convergence:
  `V(s) = E[ r + Œ≥ * V(s‚Ä≤) ]`  
- **Policy Improvement**:  
  Update the policy by acting greedily with respect to the current value function:  
  `œÄ‚Ä≤(s) = argmax_a [ r + Œ≥ * V(s‚Ä≤) ]`  
- The new policy replaces the old one, and the cycle repeats.  
- Stops when the policy no longer changes‚Äîi.e., it's optimal.  

---

## üß∞ Use Cases

- Solving small to medium-sized MDPs when the environment's model (transition probabilities and rewards) is known.  
- Benchmarking and theoretical study of reinforcement learning algorithms.  
- Teaching and understanding the concept of policy evaluation and improvement.  

---

## ‚úÖ Pros

- Converges to the optimal policy in finite steps.  
- More sample-efficient than some value-only methods like [[Value Iteration]] in small environments.  
- Clear and interpretable learning process.  

---

## ‚ùå Cons

- Requires access to a **complete model** of the environment (transition and reward functions).  
- Computationally expensive in large state/action spaces due to repeated evaluations.  
- Not suitable for model-free or online learning without modification.  

---

## üìä Comparison Table: Policy Iteration vs Related Methods

| Algorithm         | Requires Model | Value Estimation | Policy Update       | Best Use Case           |
|-------------------|----------------|------------------|----------------------|--------------------------|
| Policy Iteration  | ‚úÖ Yes         | Full evaluation  | Greedy improvement   | Small MDPs               |
| Value Iteration   | ‚úÖ Yes         | Bootstrapped     | Simultaneous         | Faster convergence       |
| Q-Learning        | ‚ùå No          | Bootstrapped     | Off-policy           | Model-free RL            |
| SARSA             | ‚ùå No          | Bootstrapped     | On-policy            | Exploration-focused tasks|
| PPO               | ‚ùå No          | Approximate      | Gradient ascent      | Large/continuous spaces  |

---

## üîß Compatible Items

- [[Bellman Equation]] ‚Äì Core to policy evaluation  
- [[Value Function]] ‚Äì Evaluated and used to improve policy  
- [[MDP]] (Markov Decision Process) ‚Äì The formal setting for Policy Iteration  
- [[Policy]] ‚Äì The primary object being updated  
- [[Dynamic Programming]] ‚Äì Category where Policy Iteration belongs  
- [[Value Iteration]] ‚Äì Related but performs simultaneous updates  

---

## üîó Related Concepts

- [[Q-Learning]] (Model-free alternative using TD updates)  
- [[Policy Gradient]] (Optimization-based approach to policy learning)  
- [[Temporal Difference Learning]] (Basis for approximated evaluation)  
- [[Actor Critic]] (Combines policy improvement and value estimation)  
- [[Policy Evaluation]] ‚Äì First step of Policy Iteration  

---

## üìö Further Reading

- [Sutton & Barto ‚Äì Chapter 4: Dynamic Programming](http://incompleteideas.net/book/the-book.html)  
- [David Silver's RL Course ‚Äì Lecture 2](https://www.davidsilver.uk/teaching/)  
- [Wikipedia ‚Äì Policy Iteration](https://en.wikipedia.org/wiki/Policy_iteration)  
- [CS50 AI: Policy Iteration](https://cs50.harvard.edu/ai/2020/weeks/7/)  

---
