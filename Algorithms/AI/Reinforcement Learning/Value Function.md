# Value Function

A **Value Function** in reinforcement learning represents the expected return (cumulative reward) an agent can achieve from a given state or state-action pair under a certain policy. It is a core concept in RL, helping agents decide which actions to take by estimating long-term outcomes.

---

## ğŸ” Overview

- The value function quantifies **how good** it is to be in a state or to take a certain action in a state.  
- Two main types:
  - **State-value function `V(s)`**: Expected return from state `s` following policy Ï€.  
  - **Action-value function `Q(s, a)`**: Expected return from taking action `a` in state `s`, then following Ï€.  
- Central to **value-based methods** like [[Q-Learning]], [[SARSA]], and [[Dynamic Programming]].

---

## ğŸ§  Core Concepts

- **State-Value Function**:  
  `VÏ€(s) = EÏ€[ âˆ‘ Î³^t * r_t | s_0 = s ]`  
- **Action-Value Function**:  
  `QÏ€(s, a) = EÏ€[ âˆ‘ Î³^t * r_t | s_0 = s, a_0 = a ]`  
- **Discount Factor (Î³)**: A number in [0,1) that reduces the weight of future rewards.  
- Value functions can be:
  - **Tabular**: Stored explicitly for small environments.  
  - **Approximated**: With linear models or neural networks for larger environments.

---

## ğŸ§° Use Cases

- Used in policy evaluation and improvement steps in classical RL algorithms like [[Policy Iteration]] and [[Value Iteration]].  
- Guides exploration and exploitation in model-free methods.  
- Forms the critic in [[Actor Critic]] architectures.  
- Basis for bootstrapping in [[TD Learning]] and other learning strategies.

---

## âœ… Pros

- Allows the agent to make **long-term decisions** instead of greedy, short-sighted choices.  
- Enables bootstrapping (predicting from predictions) to speed up learning.  
- Efficient with proper generalization in large or continuous domains.

---

## âŒ Cons

- Requires accurate estimation, which is challenging with function approximation.  
- Incorrect value estimates can misguide policies.  
- Can lead to instability when combined with non-linear approximators like deep networks.

---

## ğŸ“Š Comparison Table: V(s) vs Q(s, a)

| Property                  | V(s)                       | Q(s, a)                     |
|---------------------------|----------------------------|-----------------------------|
| Describes                 | Value of a state           | Value of a state-action pair|
| Policy Evaluation         | âœ… Yes                     | âœ… Yes                      |
| Used in Policy Improvement| âœ… Indirectly              | âœ… Directly                 |
| Needed by Actor-Critic    | âœ… Critic                  | âœ… Actor/Critic             |
| Table Size (tabular)      | |S|                        | |S| Ã— |A|                   |

---

## ğŸ”§ Compatible Items

- [[Bellman Equation]] â€“ Defines the recursive relationship for value functions  
- [[Q-Learning]] â€“ Learns the Q-value function  
- [[Policy Iteration]] â€“ Uses V(s) and policy improvement  
- [[Actor Critic]] â€“ Separates policy (actor) and value (critic)  
- [[Temporal Difference Learning]] â€“ Updates value estimates online  

---

## ğŸ”— Related Concepts

- [[Q-Value Function]] â€“ A specific type of value function  
- [[TD Learning]] â€“ Core method for value function updates  
- [[RL Policy]] â€“ Value function is defined under a policy  
- [[MDP]] (Markov Decision Process) â€“ The environment where value functions apply  
- [[Function Approximation]] â€“ Common for estimating V(s) or Q(s, a) in large domains  

---

## ğŸ“š Further Reading

- [Sutton & Barto â€“ Chapter 3: The Value Function](http://incompleteideas.net/book/the-book.html)  
- [David Silverâ€™s RL Course â€“ Lecture 2](https://www.davidsilver.uk/teaching/)  
- [Spinning Up â€“ Introduction to RL](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html)  
- [CS50 AI â€“ Value Functions](https://cs50.harvard.edu/ai/2020/weeks/7/)

---
