# MDP

A **Markov Decision Process (MDP)** is the mathematical framework that formalizes the environment in reinforcement learning. It models decision-making situations where outcomes are partly random and partly under the control of an agent. Most RL algorithms assume the environment behaves as an MDP.

---

## ğŸ” Overview

- An MDP provides a structured way to describe the dynamics of environments using a tuple:  
  `(S, A, P, R, Î³)`  
- The **Markov property** means the future state depends only on the current state and action, not on the history.  
- RL problems are typically modeled as MDPs to enable theoretical guarantees and algorithm design.

---

## ğŸ§  Core Concepts

- **States (S)**: All possible configurations of the environment.  
- **Actions (A)**: All possible decisions the agent can take.  
- **Transition Probability (P)**: `P(sâ€² | s, a)` is the probability of moving to state `sâ€²` from state `s` by taking action `a`.  
- **Reward Function (R)**: `R(s, a, sâ€²)` gives the immediate reward received when transitioning.  
- **Discount Factor (Î³)**: Determines how much future rewards are worth compared to immediate ones.  
- **Policy (Ï€)**: A strategy that defines the agentâ€™s behavior, mapping states to actions.

---

## ğŸ§° Use Cases

- Underlies all classical and modern RL algorithms.  
- Applicable in domains such as robotics, game-playing, recommendation systems, and finance.  
- Forms the basis for policy evaluation, value iteration, and dynamic programming.  

---

## âœ… Pros

- Provides a clean mathematical structure for modeling decision-making.  
- Supports convergence and optimality guarantees.  
- Modular â€” can be extended to [[POMDP]] (Partially Observable MDPs), continuous spaces, and stochastic policies.  

---

## âŒ Cons

- Real-world problems may violate the Markov assumption.  
- Requires full knowledge of transition and reward functions for model-based methods.  
- State and action spaces can grow exponentially, making exact solutions intractable.  

---

## ğŸ“Š Comparison Table: MDP vs Other Formulations

| Framework      | Observability | Transition Model | Common Use Case         |
|----------------|----------------|------------------|--------------------------|
| [[MDP]]        | Full           | Known or unknown | Standard RL setup        |
| POMDP          | Partial        | Known/unknown    | Noisy sensors, partial info |
| Bandit         | N/A            | No state         | Single-step decisions    |
| Markov Chain   | Full           | Known            | No agent actions         |

---

## ğŸ”§ Compatible Items

- [[Bellman Equation]] â€“ Defined over MDPs  
- [[Q-Learning]] â€“ Learns optimal policy from MDP interaction  
- [[Policy Iteration]] â€“ Iteratively solves MDPs  
- [[Value Function]] â€“ Describes expected return in an MDP  
- [[State Space]], [[Action Space]] â€“ Core components of MDP definition  

---

## ğŸ”— Related Concepts

- [[RL Policy]] â€“ The agentâ€™s behavior in an MDP  
- [[Reward Signal]] â€“ Defined in the MDP as part of `R(s, a, sâ€²)`  
- [[RL Transition]] â€“ The probabilistic change of state  
- [[TD Learning]] â€“ Learns value functions under MDP assumptions  
- [[Exploration vs Exploitation]] â€“ Key behavior in interacting with MDPs  

---

## ğŸ“š Further Reading

- [Sutton & Barto â€“ Chapter 3: The MDP](http://incompleteideas.net/book/the-book.html)  
- [David Silverâ€™s RL Course â€“ Lecture 2](https://www.davidsilver.uk/teaching/)  
- [Spinning Up â€“ RL Foundations](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#mdps)  
- [Wikipedia: Markov Decision Process](https://en.wikipedia.org/wiki/Markov_decision_process)  

---
