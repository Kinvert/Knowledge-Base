# Off-Policy

**Off-Policy** refers to a class of reinforcement learning algorithms that learn the value of an optimal policy independently of the agent‚Äôs actions taken in the environment. In other words, off-policy methods can learn from data generated by a different policy (behavior policy) than the one being optimized (target policy).

---

## üîç Overview

- Off-policy algorithms decouple the data collection policy from the learning policy.  
- Allow learning from previously collected experiences stored in a **Replay Buffer**.  
- Enable **experience reuse**, improving sample efficiency.  
- Commonly contrasted with **On-Policy** methods, which require fresh data from the current policy.  
- Widely used in algorithms like [[DQN]], [[DDPG]], [[SAC]], and [[TD3]].

---

## üß† Core Concepts

- **Behavior Policy**: The policy used to generate experience data.  
- **Target Policy**: The policy being improved or evaluated.  
- **Importance Sampling**: Sometimes used to correct the distribution mismatch between behavior and target policies.  
- **Replay Buffer**: Stores experiences collected under behavior policy for off-policy updates.  
- **Bootstrapping**: Updates use estimates of future values (e.g., TD learning).

---

## üß∞ Use Cases

- Learning from logged or offline data (e.g., offline RL).  
- Improving sample efficiency by reusing past experiences multiple times.  
- Environments where exploration policies differ from the learned policy.  
- Robotics, autonomous driving, and continuous control tasks where data collection is costly.

---

## ‚úÖ Pros

- Enables efficient use of data via experience replay.  
- Can learn from demonstrations or previously collected datasets.  
- Allows asynchronous learning separate from data collection.  
- Supports complex continuous and discrete control tasks.

---

## ‚ùå Cons

- May suffer from divergence or instability if behavior and target policies differ too much.  
- Requires mechanisms like importance sampling or clipped updates to ensure stable learning.  
- More complex theoretical analysis compared to on-policy methods.

---

## üìä Comparison Table: Off-Policy vs On-Policy

| Aspect                  | Off-Policy                      | On-Policy                      |
|-------------------------|--------------------------------|-------------------------------|
| Data Requirement        | Reuses past experience          | Requires fresh data from current policy |
| Sample Efficiency       | High                           | Lower                        |
| Stability               | Can be unstable without corrections | Generally stable             |
| Examples                | DQN, DDPG, SAC, TD3            | PPO, A2C, A3C                |
| Exploration             | Behavior and target policies differ | Behavior = target policy     |

---

## üîß Compatible Items

- [[Replay Buffer]] ‚Äì Core component enabling off-policy learning  
- [[Experience Replay]] ‚Äì Technique to utilize past data  
- [[Importance Sampling]] ‚Äì Corrects distribution mismatch in some methods  
- [[Deep Q Learning]] ‚Äì Classic off-policy algorithm  
- [[Actor Critic]] ‚Äì Off-policy variants like DDPG, SAC  

---

## üîó Related Concepts

- [[On-Policy]] ‚Äì Complementary RL paradigm  
- [[Bootstrapping]] ‚Äì Core to many off-policy value updates  
- [[Importance Sampling]] ‚Äì Statistical correction technique  
- [[Replay Buffer Sampling]] ‚Äì Methods to sample data for off-policy updates  
- [[Policy Evaluation]] ‚Äì Estimating the value of target policy  

---

## üìö Further Reading

- [Reinforcement Learning: An Introduction - Sutton & Barto (Chapters on Off-Policy Learning)](http://incompleteideas.net/book/the-book-2nd.html)  
- [DQN Paper - Mnih et al., 2015](https://www.nature.com/articles/nature14236)  
- [DDPG Paper - Lillicrap et al., 2015](https://arxiv.org/abs/1509.02971)  
- [SAC Paper - Haarnoja et al., 2018](https://arxiv.org/abs/1801.01290)  

---
