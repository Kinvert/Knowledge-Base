# SKRL

SKRL (Scalable Reinforcement Learning) is an open-source library designed to provide modular, scalable, and hardware-accelerated implementations of state-of-the-art Reinforcement Learning (RL) algorithms. Built with flexibility in mind, SKRL supports PyTorch and NVIDIA‚Äôs Isaac Gym and Omniverse Isaac Sim, making it particularly well-suited for robotics and simulation environments.

---

## üß† Overview

SKRL offers plug-and-play RL algorithms and utilities to accelerate development in research and industrial robotics. It prioritizes ease of use, GPU acceleration, and compatibility with robotics simulators. Developers can easily swap environments, policies, and backends to suit different RL workflows.

---

## üìò Core Concepts

- **Policy and Value Models:** Modular design supports independent customization and architecture swapping.
- **Environments:** Supports Gym-like APIs and native integration with NVIDIA‚Äôs Isaac platforms.
- **Accelerated Training:** Leverages GPU acceleration for high-speed training in simulation.
- **Logging and Evaluation:** Built-in evaluation and logging tools including [[WandB]] and TensorBoard support.

---

## üõ†Ô∏è Key Features

- Modular architecture with support for both discrete and continuous action spaces
- Multiple backend compatibility (PyTorch, TorchScript)
- Native integration with Isaac Gym and Isaac Sim
- Ready-to-use implementations of popular algorithms like [[PPO]], [[DDPG]], [[SAC]], and [[TD3]]
- Support for custom environments and models
- On-policy and off-policy learning

---

## üß™ Use Cases

- Sim-to-real robotics training
- Reinforcement Learning research and benchmarking
- Robotic manipulation and locomotion tasks
- High-throughput training in simulation environments

---

## üìä Comparison Chart

| Library     | Backend      | Isaac Support | Algorithms        | Robotics Focus | Logging        |
|-------------|--------------|----------------|-------------------|----------------|----------------|
| SKRL        | PyTorch      | ‚úÖ              | PPO, SAC, TD3, DDPG | ‚úÖ              | WandB, TB       |
| [[Stable-Baselines3]] | PyTorch | ‚ùå              | PPO, A2C, DQN     | ‚ùå              | TensorBoard    |
| [[RLlib]]   | TensorFlow / PyTorch | ‚ùå       | Extensive          | ‚ö†Ô∏è (not robotics-centric) | Custom, WandB |
| [[CleanRL]] | PyTorch      | ‚ùå              | Light set          | ‚ùå              | Lightweight     |
| [[Isaac Gym Envs]] | PyTorch | ‚úÖ              | PPO, SAC           | ‚úÖ              | N/A             |

---

## ‚úÖ Strengths

- Optimized for fast simulation in Isaac Gym
- Hardware acceleration with minimal setup
- Clear modular structure
- Ideal for robotics-focused research

---

## ‚ö†Ô∏è Weaknesses

- Less mature than larger RL libraries like [[Stable-Baselines3]]
- Documentation still growing
- Smaller user community

---

## üîó Related Concepts

- [[PPO]] (Proximal Policy Optimization)
- [[SAC]] (Soft Actor-Critic)
- [[Isaac Gym]] (GPU-accelerated simulator)
- [[Omniverse Isaac Sim]]
- [[Reinforcement Learning]]
- [[WandB]] (Weights & Biases)
- [[Gymnasium]] (OpenAI Gym successor)

---

## üîß Compatible Items

- [[PyTorch]]
- [[Isaac Gym]]
- [[Isaac Sim]]
- [[WandB]]
- [[NumPy]]
- [[Gymnasium]]

---

## üåê External Resources

- [GitHub: skrl/skrl](https://github.com/Toni-SM/skrl)
- [Documentation](https://skrl.readthedocs.io)
- [Isaac Gym](https://developer.nvidia.com/isaac-gym)
- [Omniverse Isaac Sim](https://developer.nvidia.com/omniverse/isaac-sim)

---

## üìö Further Reading

- RL papers and algorithm overviews on [Papers with Code](https://paperswithcode.com/)
- NVIDIA Robotics SDKs
- Tutorials from NVIDIA and SKRL GitHub repository

---
