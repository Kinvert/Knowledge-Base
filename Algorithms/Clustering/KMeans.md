# ğŸ“Œ K-Means Clustering

**K-Means** is one of the most widely used unsupervised machine learning algorithms for partitioning data into **K distinct clusters**. It works by iteratively assigning points to clusters and updating cluster centroids to minimize within-cluster variance (sum of squared distances to the centroid).

---

## ğŸ§  Summary

- **Type**: Partitional clustering algorithm
- **Goal**: Divide data into K clusters minimizing intra-cluster distances
- **Distance Metric**: Typically Euclidean distance (but can be modified)

---

## âš™ï¸ How It Works

1ï¸âƒ£ **Initialization**: Choose K initial centroids (randomly or using heuristics like k-means++).

2ï¸âƒ£ **Assignment Step**: Assign each data point to the nearest centroid (creating Voronoi-like partitions).

3ï¸âƒ£ **Update Step**: Recalculate centroids as the mean of assigned points.

4ï¸âƒ£ **Repeat**: Continue assignment and update steps until convergence (centroids stabilize or max iterations reached).

---

## ğŸ¯ Applications

- Image segmentation (e.g., color quantization)
- Customer segmentation in marketing
- Document clustering (text mining)
- Sensor fusion (grouping detections)
- [[Point Cloud]] downsampling or segmentation
- Pattern recognition and compression

---

## ğŸ› ï¸ Common Variants

| Variant          | Description |
|------------------|-------------|
| **k-means++**    | Smarter centroid initialization to improve convergence |
| **MiniBatchKMeans** | Processes data in small batches for scalability |
| **Fuzzy c-means** | Allows soft clustering (points can belong to multiple clusters) |

---

## ğŸ“Š Comparison Table

| Feature                | K-Means             | DBSCAN               | Agglomerative Clustering | [[Voronoi Clustering]] |
|------------------------|--------------------|----------------------|-------------------------|-----------------------|
| Requires number of clusters | âœ… Yes            | âŒ No                 | âŒ No                    | âœ… Yes (via seeds)     |
| Handles arbitrary shapes | âŒ No              | âœ… Yes                | âœ… Yes                   | âŒ No                  |
| Density-aware          | âŒ No               | âœ… Yes                | âš ï¸ Depends on linkage    | âŒ No                  |
| Robust to outliers      | âŒ No               | âœ… Yes                | âš ï¸ Medium                | âŒ No                  |
| Scalability             | âœ… High             | âš ï¸ Medium             | âŒ Low                   | âš ï¸ Medium              |

---

## âœ… Strengths

- Simple and fast, especially for large datasets
- Easy to implement and understand
- Works well when clusters are spherical and evenly sized
- Scales well with dimensionality and data size

---

## âŒ Weaknesses

- Requires choosing K (number of clusters)
- Sensitive to initialization (can converge to local minima)
- Poor performance on non-convex or varying-size clusters
- Struggles with outliers
- Assumes equal variance in all directions

---

## ğŸ› ï¸ Libraries / Tools

- **scikit-learn** (`sklearn.cluster.KMeans`)
- **OpenCV**
- **MATLAB**
- **TensorFlow / PyTorch** (for custom implementations)

---

## ğŸ”— Related Topics

- [[Voronoi Clustering]]
- [[DBSCAN]]
- [[Agglomerative Clustering]]
- [[Point Cloud]]
- [[OpenCV]]

---

## ğŸŒ External References

- [scikit-learn: KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)
- [Wikipedia: K-means clustering](https://en.wikipedia.org/wiki/K-means_clustering)

---
