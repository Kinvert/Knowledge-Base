# ü§ñ Transformer (Neural Network Architecture)

The **Transformer** is a deep learning architecture introduced in 2017 by Vaswani et al. in the paper *"Attention is All You Need"*. It revolutionized natural language processing (NLP) and beyond by relying entirely on self-attention mechanisms rather than recurrence or convolution.

---

## üß† Summary

- **Type**: Neural network architecture
- **Initial purpose**: Sequence modeling, machine translation
- **Key feature**: Uses self-attention to process entire sequences in parallel
- **Initial release**: 2017 (via the seminal paper)
- **Frameworks**: Implemented in [[TensorFlow]], [[PyTorch]], [[Keras]], [[ONNX]] and others

---

## üéØ Main Features

- **Self-attention**: Learns contextual relationships between words or tokens in a sequence
- **Parallelization**: Unlike RNNs, processes all tokens at once, allowing faster training
- **Scalability**: Forms the basis of large-scale models like BERT, GPT, T5
- **Positional encoding**: Adds information about token position, since attention lacks inherent sequence order

---

## üî¨ Common Use Cases

- Machine translation
- Text summarization
- Question answering
- Speech recognition
- Image classification (Vision Transformer - ViT)
- Multi-modal learning (e.g. combining text + vision)

---

## üìä Comparison Table

| Feature                    | Transformer                  | RNN / LSTM                    | CNN (for sequence tasks)         |
|----------------------------|-----------------------------|------------------------------|----------------------------------|
| Sequential processing       | ‚ùå No (parallelizable)        | ‚úÖ Yes                         | ‚ö†Ô∏è Limited                      |
| Long-range dependency model | ‚úÖ Excellent (via attention)  | ‚ö†Ô∏è Challenging                | ‚ö†Ô∏è Limited                      |
| Computation cost (training) | ‚ö†Ô∏è Higher                    | ‚úÖ Lower (per step)            | ‚úÖ Lower                        |
| Flexibility (input types)    | ‚úÖ High (can generalize)      | ‚ö†Ô∏è Mostly sequential data      | ‚ö†Ô∏è Mostly grid-like data        |

---

## ‚úÖ Strengths

- Captures long-range dependencies effectively
- Highly parallelizable ‚Üí faster training on modern hardware (e.g. GPUs, TPUs)
- Backbone for state-of-the-art models in NLP and beyond
- Flexible architecture ‚Äî adapted to vision, speech, multi-modal tasks

---

## ‚ùå Weaknesses

- Large memory and compute requirements, especially for long sequences
- Can be overkill for small datasets or simple tasks
- Needs positional encoding because it lacks sequence-awareness natively

---

## üåê External References

- [Original paper](https://arxiv.org/abs/1706.03762) ‚Äî *Attention is All You Need*
- [TensorFlow Transformer guide](https://www.tensorflow.org/text/tutorials/transformer)
- [PyTorch implementation example](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)

---

## üîó Related Notes

- [[Attention Mechanism]]
- [[Self-Attention]]
- [[BERT]]
- [[GPT]]
- [[TensorFlow]]
- [[PyTorch]]
- [[Machine Learning]]

---
