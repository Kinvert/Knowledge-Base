## Open-Source LLMs (Models)

|Model Name|Main Repository / Download Link|Notes / Hardware Support|
|---|---|---|
|Llama 2|[https://huggingface.co/meta-llama](https://huggingface.co/meta-llama)|Widely supported, optimized for consumer GPUs|
|Mistral|[https://huggingface.co/mistralai](https://huggingface.co/mistralai)|Fast, efficient, multiple sizes|
|Gemma 2|[https://huggingface.co/google/gemma-2b](https://huggingface.co/google/gemma-2b)|9B/27B params, optimized for NVIDIA GPUs|
|GPT-J|[https://huggingface.co/EleutherAI/gpt-j-6B](https://huggingface.co/EleutherAI/gpt-j-6B)|6B params, good for mid-range GPUs|
|BLOOM|[https://huggingface.co/bigscience/bloom](https://huggingface.co/bigscience/bloom)|Multilingual, scalable|
|Falcon|[https://huggingface.co/tiiuae/falcon-7b](https://huggingface.co/tiiuae/falcon-7b)|Efficient, open weights|
|Vicuna|[https://huggingface.co/lmsys/vicuna-7b](https://huggingface.co/lmsys/vicuna-7b)|Chat-optimized, Llama-based|
|RWKV|[https://github.com/BlinkDL/RWKV](https://github.com/BlinkDL/RWKV)|RNN/Transformer hybrid, low memory use|
|StarCoder|[https://huggingface.co/bigcode/starcoderbase](https://huggingface.co/bigcode/starcoderbase)|Code generation, performant|
|Dolly|[https://huggingface.co/databricks/dolly-v2-12b](https://huggingface.co/databricks/dolly-v2-12b)|Open, conversational fine-tuning|
|Gemma-2-2b-it|[https://huggingface.co/google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it)|Small, fast, easy to run locally|

## Open-Source Frameworks for Running LLMs Locally

|Framework|Main Link / Repo|Features / Notes|
|---|---|---|
|Ollama|[https://ollama.com/](https://ollama.com/)|Easiest way to run LLMs locally, GPU support|
|LM Studio|[https://lmstudio.ai/](https://lmstudio.ai/)|GUI for running models on your hardware|
|llama.cpp|[https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)|C++ implementation, runs Llama models on CPU/GPU|
|LocalAI|[https://github.com/go-skynet/LocalAI](https://github.com/go-skynet/LocalAI)|OpenAI-compatible API, runs many models|
|GPUStack|[https://github.com/gpustack/gpustack](https://github.com/gpustack/gpustack)|Cluster manager for multi-GPU setups|
|NVIDIA Triton|[https://github.com/triton-inference-server/server](https://github.com/triton-inference-server/server)|Scalable, production-grade inference, GPU only|
|Docker|[https://www.docker.com/](https://www.docker.com/)|Containerize LLM frameworks for easy deployment|
|Hugging Face Transformers|[https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)|Python, supports most open models|
## Additional Resources

- [Ollama Docker Image](https://hub.docker.com/r/ollama/ollama)
    
- [LM Studio Model Zoo](https://lmstudio.ai/models)
    
- [GPUStack Documentation](https://docs.gpustack.ai/)
    
- [Hugging Face Model Hub](https://huggingface.co/models)
