# Local LLMs (for coding)

Local Large Language Models (LLMs) are increasingly viable for coding tasks â€” generating, completing, refactoring, or explaining code â€” on onâ€‘premise hardware. With modern quantization and inference toolkits, many models that were once cloudâ€‘only can now run on consumer or prosumer GPUs. This note surveys the main options, their hardware requirements, performance tradeoffs, and how they compare to stateâ€‘ofâ€‘theâ€‘art closed models for coding tasks.

---

## ğŸ” Overview

Running LLMs locally gives you privacy, offline capability, and control over latency and cost. For coding work â€” where you may feed in private repositories, test snippets, or iterate interactively â€” local LLMs are especially attractive.  

Recent advances (better quantization â€” Q4, Q5, Q6; improved backends like `llama.cpp`, `Ollama`, `LM Studio`, `KoboldCpp`) and wellâ€‘tuned codeâ€‘specialized LLMs (e.g., CodeLlama, DeepSeekâ€‘Coder, StarCoder2, and newer community models) make this feasible without needing a datacenter-grade cluster.

But there are tradeoffs: quality vs hardware demand, context window size, quantization effects, and inference speed.  

In the sections below youâ€™ll find a breakdown of the most relevant models, what hardware you need, how they perform on coding benchmarks, and how they compare to closed-source giants like ChatGPT / Claude.

---

## ğŸ§  Core Concepts & Requirements

- **Model size (parameters):** Larger models (e.g., 34B, 33B) generally yield better code quality or reasoning, but require more VRAM / RAM.  
- **Quantization:** Reducing model precision (e.g., Q4_K_M, Q5_K_M, Q8_0) dramatically reduces memory footprint and VRAM load, at the cost of some quality. Quantization enables many to run on consumer GPUs.
- **Context window / KV cache:** Long context allows the model to understand larger codebases or multiple files; but longer windows increase memory use.  
- **Inference backend:** Tools like `llama.cpp`, `Ollama`, `LM Studio`, `KoboldCpp`, `vLLM`, etc., make local inference possible. Some support GPU acceleration (CUDA / Vulkan / ROCm), multi-GPU, or CPU fallback.
- **Hardware bottlenecks:** VRAM size is usually the limiting factor. When a model doesnâ€™t fit, backends may spill weights/KV cache to system RAM â€” but that hurts performance severely.
- **Benchmark metrics:** For coding, common metrics are pass@1 on benchmarks like [[HumanEval]], [[MBPP]], DS-1000 (data science), MultiPLâ€‘E (multilingual), or DevEval (real-world code completion/infilling).

---

## ğŸ“Š Models Comparison â€” Key Coding LLMs for Local Use

| Model (size / variant) | Typical VRAM (quantized) | Inference Speed* (on a ~12â€“24â€¯GB GPU) | Coding Benchmarks (pass@1 / comments) | Strengths / Notes |
|------------------------|--------------------------|----------------------------------------|----------------------------------------|------------------|
| **CodeLlama 7B / 13B / 34B** | 7B: ~6â€“8â€¯GB<br>13B: ~9â€“12â€¯GB<br>34B: ~16â€“24â€¯GB (Q4_K_M / Q5_K_M) | 7B: ~70â€“90 toks/s<br>13B: ~40â€“55 toks/s<br>34B: ~15â€“25 toks/s (quantized) | ~50â€“70% (varies) on HumanEval depending on size / tuning | Official codeâ€‘specialized LLM from Meta; good balance of performance and VRAM. 13B often sweetâ€‘spot for typical dev rigs. |
| **DeepSeek-Coder 6.7B / 33B** | 6.7B: ~8â€“10â€¯GB<br>33B: ~24â€“32â€¯GB+ | 6.7B: comparable to CodeLlama 7B<br>33B: slower, but usable on 24â€¯GB+ GPU | 6.7B: ~64% HumanEval; 33B: ~73â€“74% on HumanEval, MBPP, DS-1000 | Among best open-source code LLMs; 33B often outperforms CodeLlama-34B. Strong multilingual + data-science code support. |
| **StarCoder2 3B / 7B / 15B** | 3B: ~4â€“5â€¯GB<br>7B: ~6â€“8â€¯GB<br>15B: ~12â€“16â€¯GB (quantized) | 7B: ~60â€“80â€¯toks/s<br>15B: ~25â€“40â€¯toks/s | 3B: best among small models; 7B: medium-class strong performer; 15B: matches or exceeds CodeLlama 34B on some benchmarks | Very efficient for smaller GPUs; good for quick completions or chat-based coding assistance. Great open-source project (BigCode). |
| **(Emerging) Olmo3 7B** | Likely similar to other 7B models (â‰ˆâ€¯6â€“8â€¯GB) â€” community reports of local runs. | Not widely benchmarked yet, but expected similar to StarCoder2â€‘7B or CodeLlama-7B | No public large-scale published coding benchmark (as of 2025) | Newer architecture; some community interest. Might be worth watching, especially for efficient inference. |

\* Speeds are approximate and depend heavily on context window, quantization, GPU, backend.

---

## âœ… Use Cases â€” Where Local Coding LLMs Shine

- **Interactive coding assistance:** autocomplete, inline docstring generation, quick function stubs, refactoring help.  
- **Privacy-sensitive codebases:** local inference ensures your proprietary code never leaves your machine.  
- **Rapid prototyping / small scripting:** quick generation of small scripts, data analysis notebooks, automation tooling.  
- **Offline development setup / remote environments:** Linux server running LLM locally, no internet needed.  
- **Long-context code reasoning:** 13Bâ€“33B models with 16K+ context windows can handle multi-file contexts or long files.  

---

## ğŸ† Strengths & ğŸ› ï¸ Weaknesses (Pros & Cons)

### âœ… Strengths / Pros

- Full control over data â€” no external API, no privacy concerns.  
- No perâ€‘query cost (after GPU purchase).  
- Good real-world performance: several open models now rival older closed models (e.g., DeepSeekâ€‘Coder-33B vs GPT-3.5).
- Flexibility: can fineâ€‘tune, quantize, host on your own server, or integrate into local tools.  
- Versatility: many models support multiple languages, long contexts, code explanation, infilling, and more.  

### âŒ Weaknesses / Cons

- **Hardware constraints:** VRAM is always the limiting factor. Large models (30B+) may require 24â€¯GB+ and often 32â€“48â€¯GB VRAM, or multiâ€‘GPU setups.
- **Speed/performance tradeoffs:** when models donâ€™t fully fit in VRAM, inference slows drastically (spill to RAM).
- **Quality gap vs closed SOTA:** while top open models are strong, they still lag behind cutting-edge closed models in reasoning, long-horizon planning, very complex code tasks, or code bases requiring deep project-level context.  
- **Licensing / commercial restrictions:** some open models have non-commercial or restrictive licenses.  
- **Setup complexity:** tooling (quantization, model conversion, backend config) can be nontrivial if you want optimal performance.  

---

## ğŸ”„ How They Compare to Closed SOTA (ChatGPT, Claude, etc.)

- Closed models (e.g., GPT-4 / GPT-4o, Claude Opus) still generally excel at complex reasoning, sophisticated code generation across multiple files/modules, and tasks requiring real-world context (e.g., integrating with APIs, architecture-level design, natural language requirements).  
- Among open models, **DeepSeek-Coder-33B** now approaches GPT-3.5-level performance on many coding benchmarks.
- Smaller models (7B to 15B), like StarCoder2 or CodeLlama, provide a smoother, more responsive local experience â€” ideal for quick tasks, refactoring, autocompletes, or when working on limited VRAM hardware.  
- For many everyday coding tasks (scripts, data pipelines, tool automation), a 13B model may provide â€œgood enoughâ€ output, especially with good prompting and human review.  

In short: local LLMs are very competitive for many developer workflows â€” but for large-scale software architecture or advanced reasoning, closed cloud models still often win.

---

## ğŸ–¥ï¸ Running Local LLMs â€” Setup & Hardware Guidance

### ğŸ”§ Summary Hardware/Software Requirements

- **Minimum plausible setup (for small models):** 8â€“10â€¯GB VRAM (e.g., consumer GPU like RTX 3060, 4060) + 16â€“32â€¯GB system RAM. Enough for 3Bâ€“7B models (with quantization).
- **Recommended for 13Bâ€“15B:** ~12â€“16â€¯GB VRAM (e.g., 3060 Ti, 4070 / 4070 Ti / 4070 Ti Super), 32â€“64â€¯GB RAM.
- **For 30Bâ€“34B models:** 24â€“32â€¯GB VRAM (e.g., 3090, 4090, A6000), 64+â€¯GB RAM, NVMe SSD, fast CPU (8+ cores).
- **For 70B+ or multi-file heavy workloads:** workstation or small server-grade GPU(s) (e.g., dual H100, multiâ€‘GPU setup) â€” or use multi-GPU / offloading/backing strategies.

### ğŸ’» Tools & Backends Frequently Used

- **LM Studio:** GUI tool that supports many GGML-based or quantized LLMs (LLaMA, CodeLlama, StarCoder, MPT, etc.). Works with NVIDIA / AMD GPUs. For small GPUs (6â€“8â€¯GB VRAM), entry-level models; for mid-range GPUs (8â€“12â€¯GB), 7â€“14B models; for 24â€¯GB+, heavier models.
- **Ollama:** CLI / server-based system for local hosting + inference + API, good for building coding assistants or integrating with IDEs. Supports GPU (CUDA) or CPU fallback.
- **llama.cpp / KoboldCpp / vLLM / other backends:** For advanced users who want minimal overhead, custom quantization, multi-GPU, or optimized performance.

### ğŸš€ Typical Workflow (Example)

1. Pick model (e.g., CodeLlama-13B-instruct) â†’ quantize with Q4_K_M (or download community GGUF)  
2. Load in LM Studio or Ollama with GPU acceleration enabled  
3. Optionally expose a local API (useful for IDE integration, code generation scripts, agents)
4. For larger models that donâ€™t fit fully, configure offloading (to RAM or CPU), but expect slower token-per-second speeds.  

---

## ğŸ“š Coding LLMs â€” Use Cases, Strengths & Weaknesses by Model

### **CodeLlama**

- âœ… Great for developers who want a â€œset-and-forgetâ€ code model that works out of the box.  
- âœ… 13B is a good sweet-spot on many dev rigs; 7B is snappy for quick completions.  
- âš ï¸ For very large codebases, 34B may be needed â€” but that requires beefy hardware.  
- âš ï¸ Performance is decent but lags behind the top open-code models (e.g., DeepSeek) on hardest tasks.  

### **DeepSeek-Coder**

- âœ… Top open-source model for code as of 2025. 33B version often outperforms CodeLlama-34B.
- âœ… Good for data science code, library-heavy tasks, multilingual code.  
- âœ… 6.7B variant is useful if hardware is limited â€” reasonable performance with modest VRAM.  
- âš ï¸ 33B requires stronger GPU + more RAM. Also, quantization/performance tradeoffs if context is large.  

### **StarCoder2**

- âœ… Ideal for smaller GPUs or quick, frequent coding completions.  
- âœ… In the 15B class, it punches above weight â€” comparable to 34B CodeLlama in many cases.
- âœ… Good open-source license (BigCode), wide language coverage, good for learning, prototyping, or scripting tasks.  
- âš ï¸ For heavy-duty project-level coding or very long context, may be outclassed by larger models.  

### **Emerging / Experimental (e.g. Olmo3)**

- â„¹ï¸ Promising newer architectures, potentially more efficient. Community interest is rising.
- âš ï¸ Not yet widely benchmarked for coding tasks â€” uncertain quality/stability compared to mature code LLMs.  

---

## ğŸ”— Related Concepts / Other Notes

- [[HumanEval]] (benchmark for Python code generation)  
- [[MBPP]] (Another popular code generation benchmark)  
- [[StarCoder2]]  
- [[CodeLlama]]  
- [[DeepSeek-Coder]]  
- [[llama.cpp]] / [[Ollama]] / [[LM Studio]] (tools for local LLM inference)  
- [[Quantization]] (Q4, Q5, Q6, etc.) â€” critical for running models locally
- [[Olmo3]]

---

## ğŸ—ï¸ What This Means for Developer Workflows

- For many dayâ€‘to-day tasks â€” small scripts, prototyping, boilerplate generation, refactoring â€” a local 7B or 13B model (CodeLlama, StarCoder2, or DeepSeekâ€‘Coder) is often â€œgood enough,â€ with the benefit of instant, private, offline access.  
- If you work on larger codebases, dataâ€‘science pipelines, or crossâ€‘file refactoring and want higher correctness, opting for a 30B+ model may pay off â€” provided you have the hardware.  
- Local LLMs are especially compelling when integrated into your dev environment: IDE plugins, local API servers, automation tools, etc., giving nearâ€‘instant completion/refactor cycles without hitting cloud APIs.  

---

## ğŸ“– Further Reading / Resources

- Running LLMs locally â€” general guide and setup instructions.
- LM Studio GPU / VRAM requirements for different model sizes.
- DeepSeek-Coder performance and benchmarks.
- StarCoder2 paper and benchmark results.
- Community discussion and experience around new models like Olmo 3.

