# ğŸŸ£ PettingZoo

**PettingZoo** is a Python library for **multi-agent reinforcement learning (MARL)** environments. It provides a unified API to access a wide variety of multi-agent environments in a way that mirrors the structure of [[OpenAI Gym]], making it easy to switch between environments and compatible with common RL libraries like [[Stable Baselines3]], [[RLlib]], and [[Supersuit]].

---

## ğŸ§  Summary

- A standard API for **multi-agent** environments.
- Inspired by [[OpenAI Gym]], but supports more than one agent.
- Compatible with vectorization, wrappers, and monitoring tools.
- Developed and maintained by Farama Foundation.

---

## ğŸ“¦ Environment Categories

| Category       | Description                                                      | Example Environments                    |
|----------------|------------------------------------------------------------------|------------------------------------------|
| `atari`        | Multi-agent Atari games via ROM hacks                            | `pong_v2`, `boxing_v1`, `space_invaders` |
| `butterfly`    | Cooperative social dilemmas and coordination tasks               | `knights_archers_zombies_v10`           |
| `classic`      | Traditional board/card games                                     | `chess_v5`, `connect_four_v3`           |
| `magent`       | Large-scale agent simulations (via MAgent backend)               | `battle_v3`, `adversarial_pursuit_v3`   |
| `mpe`          | Multi-agent particle environments                                | `simple_spread_v2`, `simple_tag_v2`     |
| `sisl`         | Safety-oriented RL tasks                                         | `multiwalker_v7`                        |
| `toy_text`     | Simple text-based games                                          | `prison_v3`, `rps_v2`                   |

---

## ğŸ§ª How It Works

PettingZoo follows an **Agent Environment Cycle (AEC)**:

1. Environment initialized.
2. Each agent takes turns making decisions.
3. Environment steps forward when all agents have acted.

This differs from Gym's vectorized simultaneous actions approach but matches many real-world multi-agent scenarios.

---

## ğŸ› ï¸ Common Tools and Integration

- **[[Supersuit]]** â€“ Wrappers for preprocessing PettingZoo environments.
- **[[Stable Baselines3]]** â€“ Some limited support via wrappers or custom integration.
- **[[Ray RLlib]]** â€“ Good compatibility via parallel environment wrappers.
- **[[Gymnasium]]** â€“ PettingZoo environments can be converted to Gym-compatible formats.

---

## ğŸ” API Structure

Each environment exposes:

- `env.reset()`
- `env.step(action)`
- `env.render()`
- `env.observe(agent)`
- `env.agent_selection`
- `env.agents`

It supports wrappers and tools in the Gym ecosystem.

---

## ğŸ§ª Comparison Table

| Feature             | PettingZoo       | [[OpenAI Gym]] | [[Gymnasium]] | [[Unity ML-Agents]] | [[Multi-Agent Particle Envs]] |
|---------------------|------------------|----------------|----------------|----------------------|-------------------------------|
| Multi-agent Support | âœ… Native         | ğŸš« Single agent | ğŸš« Single agent | âœ… Native             | âœ… Native                     |
| Standard API        | âœ… Unified        | âœ…              | âœ…              | âš ï¸ Unity-specific      | âŒ Non-standard               |
| Environment Variety | âœ… Broad          | âœ…              | âœ…              | âœ…                    | âš ï¸ Limited                   |
| Wrappers Support    | âœ… Supersuit      | âœ…              | âœ…              | ğŸš«                    | ğŸš«                           |

---

## ğŸ† Strengths

- Well-structured API for turn-based multi-agent scenarios.
- Wide variety of environments across genres.
- Supports reproducibility and wrappers (via [[Supersuit]]).
- Cross-compatibility with many RL libraries.

---

## âš ï¸ Weaknesses

- No native support for simultaneous moves (requires parallel wrapper).
- Requires rethinking standard Gym-based training pipelines.
- Some environments are more demonstration-focused than realistic.

---

## ğŸ“ˆ Use Cases

- Studying multi-agent cooperation, competition, and social dilemmas.
- Testing RL algorithms in settings with agent interdependence.
- Curriculum learning and coordination in control tasks.

---

## ğŸ”— Related Notes

- [[Reinforcement Learning]]
- [[OpenAI Gym]]
- [[Stable Baselines3]]
- [[Supersuit]]
- [[Multi-Agent Systems]]
- [[RLlib]]

---

## ğŸŒ External Resources

- [PettingZoo Docs](https://www.pettingzoo.farama.org/)
- [GitHub Repository](https://github.com/Farama-Foundation/PettingZoo)
- [Farama Foundation](https://www.farama.org/)
- [List of Supported Environments](https://www.pettingzoo.farama.org/environments/)

---
