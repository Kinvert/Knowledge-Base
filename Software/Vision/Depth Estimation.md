# üß† Depth Estimation

**Depth Estimation** is a fundamental task in computer vision that involves determining the distance of objects from a viewpoint, typically from images or video streams. It's crucial for 3D scene understanding in applications such as robotics, autonomous vehicles, augmented reality, and medical imaging.

---

## üß∞ What Is Depth Estimation?

Depth estimation refers to calculating a per-pixel depth map ‚Äî an image where each pixel value corresponds to the distance between the camera and the object in the scene. This can be achieved via various techniques ranging from stereo vision to deep learning.

---

## üéØ Use Cases

- [[SLAM]] and navigation for robotics
- 3D reconstruction
- Object tracking and recognition
- Augmented and mixed reality (AR/MR)
- Driver assistance systems
- Gesture recognition
- Medical imaging (e.g. endoscopy)

---

## üìö Methods of Depth Estimation

### üîπ Passive Methods

| Method                 | Description |
|------------------------|-------------|
| **Monocular**          | Estimates depth from a single image using learned priors or geometry |
| **Stereo Vision**      | Uses two or more cameras to triangulate distance by disparity |
| **Structure from Motion (SfM)** | Uses multiple images from different viewpoints to recover 3D structure |
| **Depth from Defocus** | Infers depth based on image blur across focus changes |

### üîπ Active Methods

| Method                 | Description |
|------------------------|-------------|
| **Time-of-Flight (ToF)** | Measures the time light takes to reflect from objects |
| **Structured Light**     | Projects patterns and analyzes their deformation to infer depth |
| **LiDAR**                | Emits laser pulses to measure distances directly |

---

## üî¨ Monocular vs Binocular Depth Estimation

| Feature                   | Monocular                         | Binocular                          |
|---------------------------|-----------------------------------|------------------------------------|
| Input                     | Single image                      | Stereo image pair                  |
| Depth Accuracy            | Lower, relies on priors           | Higher, geometric triangulation    |
| Complexity                | Lower hardware, higher computation| Additional camera calibration       |
| Suitable For              | Mobile AR, image processing       | Robotics, driver assistance        |

See also: [[Monocular SLAM]], [[Binocular SLAM]]

---

## ‚öôÔ∏è Algorithms and Frameworks

| Name                  | Category        | Notes |
|-----------------------|-----------------|-------|
| **MiDaS**             | Deep Learning   | High-quality monocular depth estimation |
| **DPT (Dense Prediction Transformer)** | Deep Learning | Monocular depth using transformers |
| **Semi-Global Matching (SGM)** | Stereo Matching | Common in stereo pipelines like OpenCV |
| **COLMAP**            | SfM / MVS       | Photogrammetry pipeline |
| **OpenCV**            | Stereo Block Matching, SGBM | Classical stereo algorithms |
| **PyTorch3D**         | 3D estimation   | Supports differentiable rendering and depth |

---

## üõ†Ô∏è Tools & Libraries

- [[OpenCV]]
- [[PCL]] (Point Cloud Library)
- [[Open3D]]
- [[ROS2]] (Robot Operating System)
- [MiDaS](https://github.com/intel-isl/MiDaS)
- [COLMAP](https://colmap.github.io/)
- [TartanAir](https://theairlab.org/tartanair-dataset/) (for datasets)

---

## üîÑ Output Format

- **Depth Map**: 2D grayscale image with distance values
- **Point Cloud**: Set of 3D coordinates derived from depth
- **Mesh**: 3D triangulated surface generated from depth data

---

## üìä Comparison Table

| Method               | Hardware Needed    | Accuracy     | Suitable For         | Real-Time? |
|----------------------|--------------------|--------------|----------------------|------------|
| Monocular (MiDaS)    | Camera only        | Medium       | AR, offline analysis | ‚úÖ With GPU |
| Stereo (SGM)         | Two cameras        | High         | Robotics, ADAS       | ‚úÖ          |
| LiDAR                | LiDAR sensor       | Very High    | ADAS, mapping        | ‚úÖ          |
| Structured Light     | IR projector + cam | High         | Face ID, industrial  | ‚ö†Ô∏è Depends |
| Time-of-Flight       | Specialized sensor | Medium‚ÄìHigh  | Mobile devices       | ‚úÖ          |

---

## ‚úÖ Strengths

- Enables 3D understanding from 2D images
- Crucial for autonomous navigation and perception
- Can be done with or without active sensors
- Wide variety of approaches

---

## ‚ùå Limitations

- Monocular estimation is less accurate and ambiguous
- Stereo requires calibration and good lighting
- Active methods need specialized (and sometimes expensive) hardware
- Performance varies by scene texture and motion

---

## üîó Related Notes

- [[Monocular SLAM]]
- [[Binocular SLAM]]
- [[OpenCV]]
- [[PCL]]
- [[Open3D]]
- [[Point Cloud]]
- [[Camera Calibration]]
- [[ROS2]]
- [[SLAM]]

---
